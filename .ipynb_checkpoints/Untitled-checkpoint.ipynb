{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the dataset\n",
    "In `gluon`, neural networks are made out of Blocks (`gluon.Block`). Blocks are units that take inputs and generate outputs. Blocks also contain parameters we can update.\n",
    "\n",
    "\n",
    "\n",
    "Note that because our synthetic features X live on data_ctx and because our noise also lives on data_ctx, the labels y, produced by combining X and noise in real_fn also live on data_ctx\n",
    "\n",
    "We need to iterate through our data points quickly and grab batches of k data points at a time, to shuffle our data. In MXNet, data iterators give us a nice set of utilities for fetching and manipulating data. In particular, we’ll work with the simple DataLoader class, that provides an intuitive way to use an ArrayDataset for training models.\n",
    "\n",
    "````\n",
    "for param in params:\n",
    "    param.attach_grad()\n",
    "```\n",
    "`attach_grad` attaches the gradient to param\n",
    "\n",
    "We will use the mnist database to cateogize iages. We can represent the handwritten '8', 18x18 pixel, below as\n",
    "\n",
    "![handrwitten 8](./assets/8.png)\n",
    "\n",
    "![pixel_8](./assets/8_matrix.png)\n",
    "\n",
    "This image represents the same image as an 18x18 matrix, with each cell holding the greyscale value for the corresponding pixel: ‘0’ for white, ‘255’ for black and values in between for 254 shades of grey. This matrix representation is what we’ll run through a neural network to train it in categorizing digits from 0 to 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "First, we will fetch the MNIST dataset, which is an extensive data set that contains handwritten digits and their labels. Each image in the dataset has been resized into a 28x28 pixel grid with gray-scale values between 0 and 254.\n",
    "\n",
    "The following code downloads and loads the images and corresponding labels into memory.\n",
    "\n",
    "The entire MNISY dataset shoudl eb fully loaded into memory. For large datssets, it's not feasible to pre-;loda the entire dataset first liek we did here.\n",
    "\n",
    "What is needed in the case is a machnism by whichwe can quickly and efficneitly strea mdata directly from the source.\n",
    "\n",
    "MXNet Data iterators is the mexchanism by which we feed input data into an MXNET trainign algorith mand they are very sinple to initialzie and use and are optimzied of speed. During training ,we typically process trainign samples in small batches and over the entire training lifetime will end up processing each training example multiple times. \n",
    "\n",
    "Image batches are commonly represented by a 4-D array with shape (batch_size, num_channels, width, height). For the MNIST dataset, since thei mages are grayscale, there is only one color channel. Also, the images are 28x28 pixels, and so each image has width and heightequal to 28. Therefore, the shape of input is (batch_size, 1, 28, 28). Another important consideration is the order of input samples. \n",
    "\n",
    "To visualize the dataset, we can use matplotlib to plot the first 10 images and print their labels.\n",
    "We plot the first 10 images and print their labels.\n",
    "\n",
    "Next we create data iterators for MXNet. The data iterator is similar to a python iterator and returns a batch of data in each `next()` call. A batch contains several images with its according labels. These images are stored in a 4-D matrix with shape `(batch_size, num_channels, width, height)`. For the MNIST dataset, there is only one color channel, and both width and height are 28. In addition, we will shuffle the images used for training, which accelerates the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron\n",
    "A multilayer perceptron contains several fully-connected layers. A fully-connected layer, with an `n x m` input matrix X outputs a matrix Y with size `n x k`, where k is often called as the hidden size. This later has two parameters, the `m x k` weight matrix `W` and the `m x 1` bias vector `b`. It computes the outputs with `Y = WX + b`\n",
    "\n",
    "The output of a fully-connected layer is often fed into an activation layer, which performs element-wise operations. Two common options are the sigmoid function or the rectifier linear unit (ReLU) function, which outputs the max of 0 and the input.\n",
    "\n",
    "The last fully-connected layer's size to the number of classes in the dataset. Then we stack a softmax layer, which maps the input into a probability score. \n",
    "\n",
    "Let's define the multilayer perceptron in MXNet as follow\n",
    "\n",
    "MXNet has a neat method that allows us to visualize the network structure. It gives us high-level information such as the number of nodes in each layer and the activation functions between each layer.\n",
    "\n",
    "So, what have we just done? We defined the network structure and prepared out training and validation data iterators. Looks like we can start training! We're going to continually report useful metrics to see how well our model is doing with every epoch.\n",
    "\n",
    "We'd like to report the final validation accuracy because it tells us the accuracy rate on data not seen during training. If training data accuracy increase while validation data accuracy is decreasing, our model is over-fitting to the training data and we would need to decrease the complexity of our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
